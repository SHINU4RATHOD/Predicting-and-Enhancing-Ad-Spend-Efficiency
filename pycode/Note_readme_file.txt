. 1 all splitted dataset in diffrent tsv(tab separeted) files i combined all chunks of dataset into one file and i saved that file in .csv formate for feature use

2. followed code modularity written code structured manner means first import all required lib and dataset as well 
3. using sql alchemy pushed all dataset into database like mysql and then i grabed that data using python for further task
4. after that i have went though the task of data understanding like analyzing data like what are the available features and data types of all column 
and explored data with help of univariate tech, biviate tech and multivariate tech then decided required and not required column 
5. data preprocessing steps for all numerical and non numeric column first followed step to handle missing values, outliers, scaling, converting categorical variable to numerical column using OneHotEncoding tech the combined all preprocessed data into one 
6. then splitted dataset into training testing for model performance test and trained model with both training and test dataset and saved best model in pickle formate so that file can be used in feature for to test new data unseen data not seen by our model 
7. finally i have imported all data pipeline and model pipeline saved file and then tested with one new sample dataset

Note: i have provied one excel file as well which having infomation about model accuracy or model evation metrics

